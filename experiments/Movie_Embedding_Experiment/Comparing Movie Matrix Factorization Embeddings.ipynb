{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be difficult to quantify the impact of changing a hyperparameter on an embedding model. In this notebook we use matrix factorization to train a couple of movie embedding models based on the MovieLens dataset, and then use embedding comparison to see how changing the value of hyperparameters affects the learned embedding spaces.\n",
    "\n",
    "We also show how certain hyperparameter settings for WALS create embedding spaces very similar to those that we would learn with SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "from urllib import urlretrieve\n",
    "\n",
    "from experiment_helpers import (\n",
    "    get_embeddings_from_ratings_df,\n",
    "    compare_embedding_maps\n",
    ")\n",
    "from repcomp.comparison import CCAComparison, UnitMatchComparison, NeighborsComparison\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "logging.getLogger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data (MovieLens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../../data\"\n",
    "\n",
    "# dataset = \"ml-1m\"\n",
    "dataset = \"ml-20m\"\n",
    "\n",
    "clear_command = \"rm -rf {}/{}\".format(data_path, dataset)\n",
    "os.system(clear_command)\n",
    "\n",
    "urlretrieve(\"http://files.grouplens.org/datasets/movielens/{}.zip\".format(dataset),\n",
    "            \"{}/{}.zip\".format(data_path, dataset))\n",
    "\n",
    "unzip_command = \"unzip {}/{}.zip  -d {}\".format(data_path, dataset, data_path)\n",
    "subprocess.check_output(unzip_command, shell=True)\n",
    "\n",
    "headers = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "if dataset == \"ml-1m\":  \n",
    "    ratings_df  = pd.read_csv(\"{}/{}/ratings.dat\".format(data_path, dataset),\n",
    "                  delimiter=\"::\", header=None, names=headers)\n",
    "    \n",
    "    # Load the movie titles\n",
    "    movie_df = pd.read_csv(\"{}/{}/movies.dat\".format(data_path, dataset), delimiter=\"::\", header=None)\n",
    "    id_to_title = dict(zip(movie_df[0].values, zip(movie_df[1].values, movie_df[2].values)))\n",
    "elif dataset == \"ml-20m\":\n",
    "    ratings_df  = pd.read_csv(\"{}/{}/ratings.csv\".format(data_path, dataset),\n",
    "                  delimiter=\",\", header=0, names=headers)\n",
    "\n",
    "    # Load the movie titles\n",
    "    movie_df = pd.read_csv(\"{}/{}/movies.csv\".format(data_path, dataset))\n",
    "    id_to_title = dict(zip(movie_df['movieId'].values, zip(movie_df['title'].values, movie_df['genres'].values)))\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "ratings_df['item_id'] = [id_to_title[item_id] for item_id in ratings_df['item_id'].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train example WALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from repcomp.neighbors import get_neighbors_table\n",
    "\n",
    "# Train the matrix factorization model\n",
    "movie_to_embedding = get_embeddings_from_ratings_df(\n",
    "    ratings_df, algorithm=\"wals\", latent_factors=10, unobs_weight=0.01)[1]\n",
    "\n",
    "# Look at the nearest neighbors by movie\n",
    "movie_names = list(movie_to_embedding.keys())\n",
    "movie_embeddings = np.vstack(movie_to_embedding.values())\n",
    "table =  get_neighbors_table(movie_embeddings, \"brute\")\n",
    "for index in np.random.permutation(range(len(movie_to_embedding)))[:3]:\n",
    "    neighbor_indices = table.get_neighbors(index, 3)\n",
    "    print(\"Query: {} \\nResults: \\n{}\\n ----- \\n\".format(\n",
    "        movie_names[index],\n",
    "        \"\\n\".join([str(movie_names[neighbor_index]) for neighbor_index in neighbor_indices])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the effect of a hyperparameters on the learned WALS embedding space\n",
    "One of the important hyperparameters of a WALS model is the amount of weight to assign the unobserved elements. In this experiment we look at how increasing the unobserved weights that we use to train the WALS models yields distinct movie embedding spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alg = \"wals\"\n",
    "factors = 10\n",
    "uweights = [0.0, 0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "\n",
    "baseline_movie_embedding = get_embeddings_from_ratings_df(ratings_df, alg, factors, 0.0)[1]\n",
    "retrained_movie_embeddings = [get_embeddings_from_ratings_df(ratings_df, alg, factors, uweight)[1]\n",
    "                              for uweight in uweights]\n",
    "similarities = [compare_embedding_maps(baseline_movie_embedding, ret) for ret in retrained_movie_embeddings]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Neighbor Method\")\n",
    "plt.xlabel(\"Unobserved Element Weight\")\n",
    "plt.ylabel(\"Similarity to Unobserved Weight = 0\")\n",
    "plt.plot(uweights, [s[\"neighbor\"] for s in similarities])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"CCA Method\")\n",
    "plt.xlabel(\"Unobserved Element Weight\")\n",
    "plt.ylabel(\"Similarity to Unobserved Weight = 0\")\n",
    "plt.plot(uweights, [s[\"cca\"] for s in similarities])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's train an SVD model compares to these learned WALS models. Since the loss that SVD minimizes does't bias towards all of the elements in the matrix we see that the SVD embeddings are most similar to the WALS embeddings trained with larger unobserved weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svd_movie_embedding = get_embeddings_from_ratings_df(ratings_df, \"svd\", factors, None)[1]\n",
    "svd_similarities = [compare_embedding_maps(svd_movie_embedding, ret) for ret in retrained_movie_embeddings]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(\"Neighbor Method\")\n",
    "plt.xlabel(\"Unobserved Element Weight\")\n",
    "plt.ylabel(\"Similarity to SVD Embeddings\")\n",
    "plt.plot(uweights, [s[\"neighbor\"] for s in svd_similarities])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(\"CCA Method\")\n",
    "plt.xlabel(\"Unobserved Element Weight\")\n",
    "plt.ylabel(\"Similarity to SVD Embeddings\")\n",
    "plt.plot(uweights, [s[\"cca\"] for s in svd_similarities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfakes",
   "language": "python",
   "name": "deepfakes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
