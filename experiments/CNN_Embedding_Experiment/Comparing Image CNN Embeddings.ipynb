{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use embedding comparison to measure the difference between the representations that neural network models learn. In this notebook, we compare the final-layer embeddings for Imagenet-trained VGG16, VGG19, and InceptionV3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend\n",
    "import subprocess\n",
    "import logging\n",
    "from scipy.misc import imread, imresize\n",
    "from urllib import urlretrieve\n",
    "from repcomp.comparison import CCAComparison, UnitMatchComparison, NeighborsComparison\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "logging.getLogger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../../data\"\n",
    "clear_command = \"rm -rf {}/caltech.tar.gz; rm -rf {}/101_ObjectCategories\".format(data_path, data_path)\n",
    "os.system(clear_command)\n",
    "\n",
    "urlretrieve(\"http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\",\n",
    "            \"{}/caltech.tar.gz\".format(data_path))\n",
    "\n",
    "unzip_command = \"tar xvzf {}/caltech.tar.gz -C {}\".format(data_path, data_path)\n",
    "subprocess.check_output(unzip_command, shell=True)\n",
    "categories = os.listdir(\"{}/101_ObjectCategories\".format(data_path))\n",
    "\n",
    "def load_image(path):\n",
    "    im = imresize(imread(path), (224,224, 3))\n",
    "    return np.dstack([im, im, im]) if len(im.shape) == 2 else im\n",
    "\n",
    "images = []\n",
    "image_categories = []\n",
    "for c in tqdm(categories):\n",
    "    dirpath = \"{}/101_ObjectCategories/{}\".format(data_path, c)\n",
    "    images += [load_image(os.path.join(dirpath, name)) for name in os.listdir(dirpath) if len(name)]\n",
    "    image_categories += [c] * len(images)\n",
    "imageset = np.vstack([im[None,...] for im in images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg16, vgg19, inception_v3\n",
    "\n",
    "batch_size = 100\n",
    "embeddings = {}\n",
    "for name, Model, preprocess_func in [\n",
    "        (\"vgg16\", vgg16.VGG16, vgg16.preprocess_input),\n",
    "        (\"vgg19\", vgg19.VGG19, vgg19.preprocess_input),\n",
    "        (\"inception\", inception_v3.InceptionV3, inception_v3.preprocess_input)]:\n",
    "    backend.clear_session()\n",
    "    model = Model(weights='imagenet', include_top=False)\n",
    "    img_data = preprocess_func(imageset)\n",
    "    embeddings[name] = np.vstack([model.predict(img_data[i:i + batch_size])[:,0,0,:]\n",
    "                               for i in tqdm(range(0, imageset.shape[0], batch_size))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from repcomp.comparison import CCAComparison, UnitMatchComparison, NeighborsComparison\n",
    "\n",
    "for similarity_kind, comparator in [\n",
    "        (\"Neighbors\", NeighborsComparison()),\n",
    "        (\"SVCCA\", CCAComparison(pca_components=100))\n",
    "    ]:\n",
    "    print(\"Inception to VGG16 {} Similarity: {}\".format(similarity_kind,\n",
    "        comparator.run_comparison(embeddings[\"inception\"], embeddings[\"vgg16\"])['similarity']))\n",
    "    print(\"Inception to VGG19 {} Similarity: {}\".format(similarity_kind,\n",
    "        comparator.run_comparison(embeddings[\"vgg19\"], embeddings[\"vgg16\"])['similarity']))\n",
    "    print(\"VGG16 to VGG19 {} Similarity: {}\".format(similarity_kind,\n",
    "        comparator.run_comparison(embeddings[\"vgg19\"], embeddings[\"inception\"])['similarity']))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfakes",
   "language": "python",
   "name": "deepfakes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
