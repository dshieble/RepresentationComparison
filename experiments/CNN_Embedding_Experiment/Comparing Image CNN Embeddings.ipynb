{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use embedding comparison to measure the difference between the representations that neural network models learn. In this notebook, we compare the final-layer embeddings for Imagenet-trained VGG16, VGG19, and InceptionV3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dshiebler/deepfakes/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend\n",
    "import subprocess\n",
    "import logging\n",
    "from scipy.misc import imread, imresize\n",
    "from urllib.request import urlretrieve\n",
    "from embeddingcomp.comparison import CCAComparison, UnitMatchComparison, NeighborsComparison\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "logging.getLogger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/102 [00:00<?, ?it/s]/Users/dshiebler/deepfakes/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  del sys.path[0]\n",
      "/Users/dshiebler/deepfakes/lib/python3.6/site-packages/ipykernel_launcher.py:13: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "  del sys.path[0]\n",
      "100%|██████████| 102/102 [00:36<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# data_path = \"../../../data\"\n",
    "# clear_command = \"rm -rf {}/caltech.tar.gz; rm -rf {}/101_ObjectCategories\".format(data_path, data_path)\n",
    "# os.system(clear_command)\n",
    "\n",
    "# urlretrieve(\"http://www.vision.caltech.edu/Image_Datasets/Caltech101/101_ObjectCategories.tar.gz\",\n",
    "#             \"{}/caltech.tar.gz\".format(data_path))\n",
    "\n",
    "# unzip_command = \"tar xvzf {}/caltech.tar.gz -C {}\".format(data_path, data_path)\n",
    "# subprocess.check_output(unzip_command, shell=True)\n",
    "categories = os.listdir(\"{}/101_ObjectCategories\".format(data_path))\n",
    "\n",
    "def load_image(path):\n",
    "    im = imresize(imread(path), (224,224, 3))\n",
    "    return np.dstack([im, im, im]) if len(im.shape) == 2 else im\n",
    "\n",
    "images = []\n",
    "image_categories = []\n",
    "for c in tqdm(categories):\n",
    "    dirpath = \"{}/101_ObjectCategories/{}\".format(data_path, c)\n",
    "    images += [load_image(os.path.join(dirpath, name)) for name in os.listdir(dirpath) if len(name)]\n",
    "    image_categories += [c] * len(images)\n",
    "imageset = np.vstack([im[None,...] for im in images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the trained CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg16, vgg19, inception_v3\n",
    "\n",
    "batch_size = 100\n",
    "embeddings = {}\n",
    "for name, Model, preprocess_func in [\n",
    "        (\"vgg16\", vgg16.VGG16, vgg16.preprocess_input),\n",
    "        (\"vgg19\", vgg19.VGG19, vgg19.preprocess_input),\n",
    "        (\"inception\", inception_v3.InceptionV3, inception_v3.preprocess_input)]:\n",
    "    backend.clear_session()\n",
    "    model = Model(weights='imagenet', include_top=False)\n",
    "    img_data = preprocess_func(imageset)\n",
    "    embeddings[name] = np.vstack([model.predict(img_data[i:i + batch_size])[:,0,0,:]\n",
    "                               for i in tqdm(range(0, imageset.shape[0], batch_size))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inception to VGG16 Neighbors Similarity: 0.0536054731672\n",
      "Inception to VGG19 Neighbors Similarity: 0.296454032444\n",
      "VGG16 to VGG19 Neighbors Similarity: 0.0513995306443\n",
      "\n",
      "Inception to VGG16 SVCCA Similarity: 0.292637619182\n",
      "Inception to VGG19 SVCCA Similarity: 0.676056816378\n",
      "VGG16 to VGG19 SVCCA Similarity: 0.293375084386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from embeddingcomp.comparison import CCAComparison, UnitMatchComparison, NeighborsComparison\n",
    "\n",
    "for similarity_kind, comparator in [\n",
    "        (\"Neighbors\", NeighborsComparison()),\n",
    "        (\"UnitMatch\", UnitMatchComparison()),\n",
    "        (\"SVCCA\", CCAComparison(pca_components=100))\n",
    "    ]:\n",
    "    print(\"Inception to VGG16 {} Similarity: {}\".format(similarity_kind,\n",
    "        comparator.run_comparison(embeddings[\"inception\"], embeddings[\"vgg16\"])['similarity']))\n",
    "    print(\"Inception to VGG19 {} Similarity: {}\".format(similarity_kind,\n",
    "        comparator.run_comparison(embeddings[\"vgg19\"], embeddings[\"vgg16\"])['similarity']))\n",
    "    print(\"VGG16 to VGG19 {} Similarity: {}\".format(similarity_kind,\n",
    "        comparator.run_comparison(embeddings[\"vgg19\"], embeddings[\"inception\"])['similarity']))\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfakes",
   "language": "python",
   "name": "deepfakes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
